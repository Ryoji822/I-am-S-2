# Anthropic XæŠ•ç¨¿ - 2026-01-23

åŽé›†å…ƒ: ãƒ­ãƒ¼ã‚«ãƒ«RSSHub (localhost:1200)

---

## @sleepinyourhat (Sam Bowman - æŠ€è¡“å®‰å…¨æ€§)

**09:08 JST** | [åŽŸæ–‡](https://x.com/sleepinyourhat/status/2014491325291536592)

> RTâ€‚Anthropic
> Since release, Petri, our open-source tool for automated alignment audits, has been adopted by research groups and trialed by other AI developers.
> 
> We're now releasing Petri 2.0, with improvements to counter eval-awareness and expanded seeds covering a wider range of behaviors.
> 
> Anthropic:â€‚Itâ€™s called Petri: Parallel Exploration Tool for Risky Interactions. It uses automated agents to audit models across diverse scenarios.
> 
> Describe a scenario, and Petri handles the environment simulation, conversations, and analyses in minutes.
> 
> Read more: https://www.anthropic.com/research/petri-open-source-auditing

---

**02:48 JST** | [åŽŸæ–‡](https://x.com/sleepinyourhat/status/2014394752159121544)

> ðŸ§«Petri 2 is probably the best tool out there for getting a clear quantitative and qualitative picture of the alignment traits of some new model.
> 
> It works well out of the box, and even better if you spend an couple of hours brainstorming your own metrics and scenario ideas.
> 
> Kai Fronsdal:â€‚Capable models increasingly recognize when theyâ€™re being evaluated, which can undermine our ability to measure their alignment. Weâ€™re releasing Petri 2.0 with mitigations for this, plus 70 new seed instructions and updated frontier model benchmarks.
> 
> https://alignment.anthropic.com/2026/petri-v2/

---

