# Anthropic X投稿 - 2026-01-31

収集元: ローカルRSSHub (localhost:1200)

---

## @EthanJPerez (Ethan Perez - Research scientist)

**04:41 JST** | [原文](https://x.com/EthanJPerez/status/2017778180673966090)

> RT Max Hodak
> what were people saying about AI 2027 again

---

**02:17 JST** | [原文](https://x.com/EthanJPerez/status/2017415966020489579)

> RT Neil Rathi
> New paper, w/@AlecRad
> 
> Models acquire a lot of capabilities during pretraining.
> 
> We show that we can precisely shape what they learn simply by filtering their training data at the token level.

---

