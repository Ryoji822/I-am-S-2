# OpenAI X投稿 - 2026-01-21

収集元: ローカルRSSHub (localhost:1200)

---

## @polynoamial (Noam Brown - 研究者)

**23:19 JST** | [原文](https://x.com/polynoamial/status/2014434984917139589)

> RT prinz
> And now for something completely different. Introducing: prinzbench.
> 
> Over the past few months, I have been increasingly using LLMs for legal work, primarily legal research and "needle-in-the-haystack" search queries. This is economically valuable work of the kind performed by many lawyers on a daily basis.  It is also quite distinct from the benchmarks that the frontier labs typically "hill-climb" - i.e., math and coding.  And importantly, correct legal analysis is not easily verifiable. There is no code to compile, there is no mathematical proof that either works or doesn't. The only way to know whether one reached the correct legal conclusion is to be confident that: (1) no legal authorities were missed when analyzing the question; and (2) the logical analysis of the relevant legal authorities was correct. 
> 
> prinzbench is a private benchmark that aims to test LLMs' abilities to do exactly this kind of work: search the internet for relevant information and appropriately analyze it to form the correct legal conclusion. 
> 
> The benchmark comprises 33 questions (25 Legal Research and 8 Search).  Each model's response to a question was graded by me personally (without use of AI in grading) at pass@1.  For each model, the complete benchmark was run 3 times, for a maximum score of 99 (33 questions * 3).
> 
> These 33 questions are hard! I intentionally picked difficult questions from my (quite niche) area of the law for the benchmark, and ad-hoc tested them against various LLM...(truncated)

---

