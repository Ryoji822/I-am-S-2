# OpenAI X投稿 - 2026-02-06

収集元: ローカルRSSHub (localhost:1200)

---

## @kevinweil (Kevin Weil - 製品責任者)

**07:29 JST** | [原文](https://x.com/kevinweil/status/2019538877833441654)

> Coding models are amazing, but "GPT-5 lowers the cost of cell-free protein synthesis by 40%" is also pretty rad. This was all done in a robotic lab in partnership with @Ginkgo. AI + Science!
> 
> OpenAI: We worked with @Ginkgo to connect GPT-5 to an autonomous lab, so it could propose experiments, run them at scale, learn from the results, and decide what to try next. That closed loop brought protein production cost down by 40%.

---

**03:24 JST** | [原文](https://x.com/kevinweil/status/2020589802153029718)

> RT Karel
> http://x.com/i/article/2018578800792203264

---

## @bradlightcap (Brad Lightcap - 業務執行)

**03:29 JST** | [原文](https://x.com/bradlightcap/status/2019496966896644515)

> RT Alexander Embiricos
> GPT-5.3-Codex is out!
> State of the art of SWEBench Pro and TBench, and a big leap in how it _feels_:
> - Mid-turn updates tell you what it's doing
> - Steer without interrupting
> - 25% faster via inference improvements
> I've been loving it. Think you will too
> https://openai.com/index/introducing-gpt-5-3-codex/

---

## @jasonkwon (Jason Kwon - 戦略担当)

**03:45 JST** | [原文](https://x.com/jasonkwon/status/2019813456749404383)

> RT Trevor Cai
> 3 years ago, we emailed Jensen with requests for Blackwell. Today, we released GPT-5.3-Codex, a SOTA model designed for GB200-NVL72. Nitpicking ISA, simming rack designs, and tailoring our arch to the system has been a fun experience! I'm grateful to our collaborators at NVIDIA.

---

**03:14 JST** | [原文](https://x.com/jasonkwon/status/2019541191050895430)

> RT Sam Altman
> GPT-5.3-Codex is here!
> 
> *Best coding performance (57% SWE-Bench Pro, 76% TerminalBench 2.0, 64% OSWorld).
> *Mid-task steerability and live updates during tasks.
> *Faster! Less than half the tokens of 5.2-Codex for same tasks, and >25% faster per token!
> *Good computer use.

---

**03:12 JST** | [原文](https://x.com/jasonkwon/status/2019547040481488919)

> RT Matt Shumer
> I’ve had early access to GPT-5.3-Codex.
> 
> It’s a fucking monster.
> 
> Runs can go 8+ hours... and I come back to working code + live deployments.
> 
> It’s significantly more autonomous than Opus 4.5.
> 
> But it’s not all positive.
> 
> My review: https://shumer.dev/gpt53-codex-review

---

**01:15 JST** | [原文](https://x.com/jasonkwon/status/2019547059955826914)

> RT Brad Lightcap
> introducing openai frontier
> 
> a new platform for enterprises to design and run powerful agent systems for real work
> 
> companies like @HP @Intuit @Oracle @StateFarm @thermofisher and @Uber are among the first we'll work with to make agents true AI coworkers
> https://openai.com/index/introducing-openai-frontier/

---

**01:01 JST** | [原文](https://x.com/jasonkwon/status/2019509205733183558)

> RT Sam Altman
> The companies that succeed in the future are going to make very heavy use of AI. People will manage teams of agents to do very complex things.
> 
> Today we are launching Frontier, a new platform to enable these companies.

---

## @npew (Peter Welinder - 研究・技術)

**17:12 JST** | [原文](https://x.com/npew/status/2019973725475791006)

> RT will brown
> Re the codex desktop app is so good that i literally don't care about whether or not 4.6 opus is a better model

---

**09:19 JST** | [原文](https://x.com/npew/status/2019966100415692961)

> RT Ben Davis
> GPT-5.3 Codex is a massive improvement over 5.2 holy shit
> 
> Way faster, fewer tool calls, and a more accurate result...

---

**07:37 JST** | [原文](https://x.com/npew/status/2019618648076480536)

> RT Sergey Karayev
> GPT 5.3 Codex is ABSOLUTELY INCREDIBLE on our "personal SWE-Bench", which is based on gold standard PRs into our Ruby on Rails codebase.
> 
> Opus 4.6 is a tiny bit better than 4.5 but very underwhelming overall.
> 
> Will see if the vibes match the bench!
> 
> More about the benchmark: https://www.superconductor.com/blog/beta-agent-benchmarks/

---

