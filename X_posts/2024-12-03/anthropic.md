# Anthropic XæŠ•ç¨¿ - 2024-12-03

åŽé›†å…ƒ: ãƒ­ãƒ¼ã‚«ãƒ«RSSHub (localhost:1200)

---

## @dpkingma (Durk Kingma - ç ”ç©¶è€…)

**04:30 JST** | [åŽŸæ–‡](https://x.com/dpkingma/status/1863667152726675857)

> ðŸ‘‡ Great work led by Yushun (@ericzhang0410) introducing Adam-mini, a version of Adam that, surprisingly, reduces Adam's memory requirement by 50% (!), without negatively affecting convergence rates. Please read Yushun's thread for details!
> 
> Yushun Zhang:â€‚Finally finished Adam-mini!  A "mini" version of Adam that painlessly frees 50% of memory over Adam. Some highlighted features:
> 
> 1. Adam-mini saves 50% memory over Adam for all modern neural nets. This is done by removing 99.9% Adam's v (but the last 0.1% of v is essential and

---

**04:14 JST** | [åŽŸæ–‡](https://x.com/dpkingma/status/1863662926101434594)

> Great blogpost by Ruiqi (and other GDM ex-colleagues), clearly explaining the the connection between flow matching and diffusion models. Super happy they took the time to explain this topic, there's confusion on this topic, I think many will find this quite valuable!
> 
> Ruiqi Gao:â€‚A common question nowadays: Which is better, diffusion or flow matching? ðŸ¤”
> 
> Our answer: Theyâ€™re two sides of the same coin. We wrote a blog post to show how diffusion models and Gaussian flow matching are equivalent. Thatâ€™s great: It means you can use them interchangeably.

---

**03:34 JST** | [åŽŸæ–‡](https://x.com/dpkingma/status/1863663243828371875)

> RTâ€‚Sander Dieleman
> In https://arxiv.org/abs/2303.00848, @dpkingma and @RuiqiGao had suggested that noise augmentation could be used to make other likelihood-based models optimise perceptually weighted losses, like diffusion models do. So cool to see this working well in practice!
> 
> Michael Tschannen:â€‚Have you ever wondered how to train an autoregressive generative transformer on text and raw pixels, without a pretrained visual tokenizer (e.g. VQ-VAE)?
> 
> We have been pondering this during summer and developed a new model: JetFormer ðŸŒŠðŸ¤–
> 
> http://arxiv.org/abs/2411.19722
> 
> A thread ðŸ‘‡
> 
> 1/

---

