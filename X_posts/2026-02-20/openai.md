# OpenAI XæŠ•ç¨¿ - 2026-02-20

åŽé›†å…ƒ: ãƒ­ãƒ¼ã‚«ãƒ«RSSHub (localhost:1200)

---

## @gdb (Greg Brockman - å…±åŒå‰µæ¥­è€…)

**10:47 JST** | [åŽŸæ–‡](https://x.com/gdb/status/2024662197692223857)

> the inference compute available to you is increasingly going to drive overall software productivity:
> 
> Tibo:â€‚I am increasingly asked during candidate interviews how much dedicated inference compute they will have to build with Codex. 
> 
> Pairing this with usage per user growing significantly faster than the number of users, it's pretty clear that compute will be something that is scarce.

---

## @kevinweil (Kevin Weil - è£½å“è²¬ä»»è€…)

**09:06 JST** | [åŽŸæ–‡](https://x.com/kevinweil/status/2024657539816673590)

> RTâ€‚Tejal Patwardhan
> new in Nature about our wet lab evals work!
> 
> nature:â€‚AI-driven autonomous robots are coming to biology laboratories, but researchers insist that human skills remain essential
> 
> https://go.nature.com/3MCzePt

---

## @jasonkwon (Jason Kwon - æˆ¦ç•¥æ‹…å½“)

**05:59 JST** | [åŽŸæ–‡](https://x.com/jasonkwon/status/2024680895228367195)

> RTâ€‚Kate Rouch ðŸ›¡ï¸
> You can just build things
> 
> Joe Reeve - ðŸ‡¬ðŸ‡§/acc:â€‚I built an app that lets you talk to statues. Naturally, I took it for a spin at the British Museum.
> 
> Full conversations in the thread.

---

## @sama (Sam Altman - CEO)

**21:41 JST** | [åŽŸæ–‡](https://x.com/sama/status/2024826822060290508)

> Great meeting with PM @narendramodi today to talk about the incredible energy around AI in India.
> 
> India is our fastest growing market for codex globally, up 4x  in weekly users in the past 2 weeks alone.
> 
> ðŸ‡®ðŸ‡³!

---

## @kevinweil (Kevin Weil - è£½å“è²¬ä»»è€…)

**09:59 JST** | [åŽŸæ–‡](https://x.com/kevinweil/status/2024847475454980165)

> RTâ€‚Bartosz NaskrÄ™cki
> Since yesterday, I have been working with my collaborator Piotr Pokora on a problem related to log surfaces. We were trying to figure out how to search the combinatorial space of possible configurations of lines on a smooth quartic in order to maximize the so-called Chern slope. In terms of numerics, we have done many examples, and the famous Fermat quartic x^4 + y^4 + z^4 + w^4 = 0 is currently the record holder for the slope (= 8/3) for a particular configuration of 16 lines (see our paper). This was the expected maximum, which we have been trying to beat or prove for the last two years. Today I ran the problem with the top version of GPT Pro using a hefty prompt that included many details about the problem and the full text of our paper.
> 
> I received a very interesting insight: to use mixed linear programming. This approach beats brute force techniques, including simulated annealing, by far. We had not seen it ourselves, yet the model found this insight and explained how to write efficient code using SciPy.
> 
> I now realize that there are three of us in the office: two humans and one agentic system with skills and substantial computational power. Skills are becoming increasingly important, and this agentic harness produces amazing results. I feel that I have completely shifted my perspective. I still like collaborating with humans, but I delegate deep searches, bold ideation, and extensive exploration to the models. It is simply faster and more efficient....(truncated)

---

## @gdb (Greg Brockman - å…±åŒå‰µæ¥­è€…)

**07:24 JST** | [åŽŸæ–‡](https://x.com/gdb/status/2024611138760298999)

> itâ€™s a good model
> 
> Yam Peleg:â€‚gpt-5.3-codex is insane

---

