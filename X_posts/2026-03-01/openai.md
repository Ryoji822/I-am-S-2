# OpenAI X投稿 - 2026-03-01

収集元: ローカルRSSHub (localhost:1200)

---

## @sama (Sam Altman - CEO)

**05:38 JST** | [原文](https://x.com/sama/status/2027850288959525125)

> RT OpenAI
> Yesterday we reached an agreement with the Department of War for deploying advanced AI systems in classified environments, which we requested they make available to all AI companies.
> 
> We think our deployment has more guardrails than any previous agreement for classified AI deployments, including Anthropic's. Here's why: https://openai.com/index/our-agreement-with-the-department-of-war/

---

## @jasonkwon (Jason Kwon - 戦略担当)

**05:38 JST** | [原文](https://x.com/jasonkwon/status/2027855528253395261)

> RT OpenAI
> Yesterday we reached an agreement with the Department of War for deploying advanced AI systems in classified environments, which we requested they make available to all AI companies.
> 
> We think our deployment has more guardrails than any previous agreement for classified AI deployments, including Anthropic's. Here's why: https://openai.com/index/our-agreement-with-the-department-of-war/

---

**05:38 JST** | [原文](https://x.com/jasonkwon/status/2027855589381132293)

> RT Boaz Barak
> Please read this blog post. Our red lines are clear and we will protect them. Unlike other agreements, we are not working through Palantir, we are responsible for our own models and our own safety. We will have our people in place to guarantee that responsible & safe deployment.
> 
> Boaz Barak: https://openai.com/index/our-agreement-with-the-department-of-war/

---

## @polynoamial (Noam Brown - 研究者)

**05:55 JST** | [原文](https://x.com/polynoamial/status/2027850059875029335)

> For those following the DoW AI drama, I highly recommend reading this post explaining how @OpenAI approached the negotiations with the DoW.
> 
> OpenAI: Yesterday we reached an agreement with the Department of War for deploying advanced AI systems in classified environments, which we requested they make available to all AI companies.
> 
> We think our deployment has more guardrails than any previous agreement for classified AI

---

## @jasonkwon (Jason Kwon - 戦略担当)

**08:14 JST** | [原文](https://x.com/jasonkwon/status/2027892779796594860)

> RT Tibo
> Personal view. On the DoW deal, time and time again I witness how OpenAI operates thoughtfully and diplomatically when it comes to raising the bar on safety. I also believe that figuring out how to deploy powerful new technology in the pursuit of national security (not just the USA) is important.
> 
> The company deeply understands what it can and cannot reasonably have control over and focuses instead of setting the right guardrails to ensure that deployment is aligned with what is considered acceptable and safe.
> 
> I am proud of the level of transparency provided in the blog post, and hope that other companies, including Anthropic, were as transparent on prior and future deals they make.
> 
> https://openai.com/index/our-agreement-with-the-department-of-war/

---

## @jasonkwon (Jason Kwon - 戦略担当)

**09:05 JST** | [原文](https://x.com/jasonkwon/status/2027901571930157153)

> RT Dean W. Ball
> When it comes to all these contract terms and disputes about them:
> 
> I really don’t know. There may be a deal to be had there, and I put a high value on putting an end to this madness. But to know whether the deal is worth taking would require serious conversations with highly specialized lawyers, and essentially no one commenting on these issues has really done that (Alan rozenshtein exempted). I am not one of those lawyers, and this is not an area of the law I’ve spent a long time dabbling in (unlike say first amendment jurisprudence, where I’d be much more inclined to weigh in on legal matters despite not being a lawyer).
> 
> But in the end I feel this is all a distraction. The question here is pretty simple: should the United States government be able to use policy to destroy a law-abiding American company because it feels like it? My answer is no, and yours should be too. The rest is noise.

---

## @kevinweil (Kevin Weil - 製品責任者)

**07:01 JST** | [原文](https://x.com/kevinweil/status/2027933644502012077)

> RT Joshua Achiam
> A firm commitment to the principle that AGI companies have to devolve power to democracies and avoid unduly concentrating power in themselves, even when that leads to uncomfortable places, is something I will not regret.
> 
> Jeremy Howard: @jachiam0 Not sure you'll ever live this down, Joshua.

---

## @jasonkwon (Jason Kwon - 戦略担当)

**10:24 JST** | [原文](https://x.com/jasonkwon/status/2027931165802504474)

> RT Sam Altman
> Re Enforcing the SCR designation on Anthropic would be very bad for our industry and our country, and obviously their company.
> 
> We said to the DoW before and after. We said that part of the reason we were willing to do this quickly was in the hopes of de-esclation.
> 
> I feel competitive with Anthropic for sure, but successfully building safe superintelligence and widely sharing the benefits is way more important that any company competition. I believe they would do something to try to help us in the face of great injustice if we could.
> 
> We should all care very much about the precedent.
> 
> I saw in some other tweet that I must not be willing to criticize the DoW (it said something about sucking their dick too hard to be able to say anything critical, but I assume this was the intent).
> 
> To say it very clearly: I think this is a very bad decision from the DoW and I hope they reverse it. If we take heat for strongly criticizing it, so be it.

---

**09:53 JST** | [原文](https://x.com/jasonkwon/status/2027930931378720936)

> RT NatSecKatrina
> Re I would gently push back on the underlying premise that if the government agrees to a usage policy restriction, that's ironclad, but if it's just a law or policy, that's no guarantee at all.  Why would Anthropic think that their earlier usage policy forbidding surveillance was sufficient to guarantee their models could not be used for this?  
> 
> My main argument is that usage policies are only one part of a layered set of safeguards.  Here's how I think about this:
> 
> 1. The safety stack travels with the model. The Department was not asking us to modify how our models behave. Their position was, build the model however you want, refuse whatever requests you want, just don't try to govern our operational decisions through usage policies. For whatever risk surface area remains, our safety stack, refusal policies, and guardrails become another protection. And those technical controls are often more reliable than contract clauses anyway. Our contract gives us control over the models and safety stack we deploy, and the ability to improve them over time.
> 
> 2. AI experts directly involved. Instead of hoping contract language will be enough, our contract allows us to embed forward deployed engineers, commits to giving us visibility into how models are being used, and we have the ability to iterate on safety safeguards over time. If our team sees that our models aren't refusing queries they should, or there's more operational risk than we expected, our contract allows u...(truncated)

---

**09:46 JST** | [原文](https://x.com/jasonkwon/status/2027931915718955232)

> RT NatSecKatrina
> Re Anthropic has primarily been concerned with usage policies, which is because their existing classified deployments involve reduced or removed safety guardrails (making usage policies the primary safeguards in national security deployments). Usage policies, on their own, are not a guarantee of anything.  Any responsible deployment of AI in classified environments should involve layered safeguards including a prudent safety stack, limits on deployment architecture, and the direct involvement of AI experts in consequential AI use cases.  That's what we pursued in our negotiations and that's why we think the deal we made has more guardrails than any previous agreement for classified AI deployments, including Anthropic's.

---

## @polynoamial (Noam Brown - 研究者)

**09:46 JST** | [原文](https://x.com/polynoamial/status/2027926465661309026)

> RT NatSecKatrina
> Re Anthropic has primarily been concerned with usage policies, which is because their existing classified deployments involve reduced or removed safety guardrails (making usage policies the primary safeguards in national security deployments). Usage policies, on their own, are not a guarantee of anything.  Any responsible deployment of AI in classified environments should involve layered safeguards including a prudent safety stack, limits on deployment architecture, and the direct involvement of AI experts in consequential AI use cases.  That's what we pursued in our negotiations and that's why we think the deal we made has more guardrails than any previous agreement for classified AI deployments, including Anthropic's.

---

## @jasonkwon (Jason Kwon - 戦略担当)

**11:02 JST** | [原文](https://x.com/jasonkwon/status/2027944526183338205)

> RT NatSecKatrina
> A lot of the concerns about the government's "all lawful use" language seem to stem from mistrust that government will follow the laws.  At the same time, people believe that Anthropic took an important stand by insisting on contract language around their redlines.  
> 
> We cannot have it both ways.  We cannot say that the government cannot be trusted to interpret laws and contracts the right way, but also agree that Anthropic’s policy redlines, in a contract, would have been effective.  
> 
> This is why our approach has been:
> 
> Let the democratic process decide on the legality and proper use question.  The fact that people can even say that the gov has made mistakes in the past is the process in action.  The fact that we are having this discussion on twitter is part of the process.
> 
> Create a reasonable contractual framework that guides expectations and the relationship, just as much if not more than the rules themselves.
> 
> And on top of this, have the ability to build the models the way we think is safe, along with cleared FDEs to do the real world work in partnership.

---

